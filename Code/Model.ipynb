{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bfcaae4",
   "metadata": {},
   "source": [
    "This code will text the valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "122e6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all data and \n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "DB_PATH = \"data/clean_music_data.db\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load tables\n",
    "# -----------------------------\n",
    "with sqlite3.connect(DB_PATH) as conn:\n",
    "    audio_df = pd.read_sql_query(\"SELECT * FROM audio_features ORDER BY timestamp\", conn)\n",
    "    midi_df = pd.read_sql_query(\"SELECT * FROM midi_events ORDER BY timestamp\", conn)\n",
    "\n",
    "audio_columns = ['rms_db','rms_delta','centroid','rolloff','flatness','low','mid','high','spectral_flux','onset_strength']\n",
    "midi_columns = ['device_id','channel','note','velocity','cc_number','cc_value','program_number','type']\n",
    "\n",
    "# -----------------------------\n",
    "# Sort tables just in case\n",
    "# -----------------------------\n",
    "audio_df = audio_df.sort_values('timestamp').reset_index(drop=True)\n",
    "midi_df = midi_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Pointers\n",
    "# -----------------------------\n",
    "audio_idx = 0\n",
    "midi_idx = 0\n",
    "n_audio = len(audio_df)\n",
    "n_midi = len(midi_df)\n",
    "\n",
    "# -----------------------------\n",
    "# Output timeline\n",
    "# -----------------------------\n",
    "timeline_rows = []\n",
    "\n",
    "# Keep track of last known states\n",
    "last_audio = {col: None for col in audio_columns}\n",
    "last_midi = {col: None for col in midi_columns}\n",
    "\n",
    "# -----------------------------\n",
    "# All timestamps sorted\n",
    "# -----------------------------\n",
    "all_timestamps = sorted(\n",
    "    list(audio_df['timestamp']) + list(midi_df['timestamp'])\n",
    ")\n",
    "\n",
    "for ts in all_timestamps:\n",
    "    # Update audio if this timestamp matches\n",
    "    if audio_idx < n_audio and audio_df.at[audio_idx, 'timestamp'] == ts:\n",
    "        for col in audio_columns:\n",
    "            last_audio[col] = audio_df.at[audio_idx, col]\n",
    "        audio_idx += 1\n",
    "\n",
    "    # Update midi if this timestamp matches\n",
    "    if midi_idx < n_midi and midi_df.at[midi_idx, 'timestamp'] == ts:\n",
    "        for col in midi_columns:\n",
    "            last_midi[col] = midi_df.at[midi_idx, col]\n",
    "        midi_idx += 1\n",
    "\n",
    "    # Build row with current states\n",
    "    row = {'timestamp': ts}\n",
    "    row.update(last_audio)\n",
    "    row.update(last_midi)\n",
    "    timeline_rows.append(row)\n",
    "\n",
    "# -----------------------------\n",
    "# Create DataFrame\n",
    "# -----------------------------\n",
    "timeline_df = pd.DataFrame(timeline_rows)\n",
    "\n",
    "# -----------------------------\n",
    "# Check results\n",
    "# -----------------------------\n",
    "#print(\"Timeline rows:\", len(timeline_df))\n",
    "#print(timeline_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a2a8b",
   "metadata": {},
   "source": [
    "### MODEL TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b530b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-10>:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-output RF accuracy: 0.8243243243243243\n",
      "Predicted next MIDI (channel, cc_number, cc_value): [ 6. 12.  0.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -----------------------------\n",
    "# Columns\n",
    "# -----------------------------\n",
    "audio_columns = ['rms_db','rms_delta','centroid','rolloff','flatness','low','mid','high','spectral_flux','onset_strength']\n",
    "midi_columns = ['device_id','channel','note','velocity','cc_number','cc_value','program_number','type']\n",
    "\n",
    "# -----------------------------\n",
    "# Fill missing numeric values\n",
    "# -----------------------------\n",
    "timeline_df[audio_columns + midi_columns[:-1]] = timeline_df[audio_columns + midi_columns[:-1]].fillna(0)\n",
    "\n",
    "# -----------------------------\n",
    "# Encode string 'type'\n",
    "# -----------------------------\n",
    "type_encoder = LabelEncoder()\n",
    "timeline_df['type_enc'] = type_encoder.fit_transform(timeline_df['type'].fillna('none'))\n",
    "\n",
    "# -----------------------------\n",
    "# Features and targets\n",
    "# -----------------------------\n",
    "feature_columns = audio_columns + midi_columns[:-1] + ['type_enc']\n",
    "\n",
    "# Predict next MIDI events\n",
    "timeline_df['next_channel'] = timeline_df['channel'].shift(-1)\n",
    "timeline_df['next_cc_number'] = timeline_df['cc_number'].shift(-1)\n",
    "timeline_df['next_cc_value'] = timeline_df['cc_value'].shift(-1)\n",
    "\n",
    "# Drop last row (no next)\n",
    "timeline_df = timeline_df.dropna(subset=['next_channel','next_cc_number','next_cc_value'])\n",
    "\n",
    "X = timeline_df[feature_columns]\n",
    "y = timeline_df[['next_channel','next_cc_number','next_cc_value']]\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# Multi-output Random Forest\n",
    "# -----------------------------\n",
    "multi_rf = MultiOutputClassifier(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42)\n",
    ")\n",
    "multi_rf.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "score = multi_rf.score(X_test, y_test)\n",
    "print(\"Multi-output RF accuracy:\", score)\n",
    "\n",
    "# -----------------------------\n",
    "# Example prediction\n",
    "# -----------------------------\n",
    "example_row = X_test.iloc[0:1]\n",
    "pred = multi_rf.predict(example_row)\n",
    "print(\"Predicted next MIDI (channel, cc_number, cc_value):\", pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "947a0a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost multi-output accuracy: 0.8378378378378378\n",
      "Predicted next MIDI (channel, cc_number, cc_value): [ 1. 32.  0.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# -----------------------------\n",
    "# Columns\n",
    "# -----------------------------\n",
    "audio_columns = [\n",
    "    'rms_db','rms_delta','centroid','rolloff','flatness',\n",
    "    'low','mid','high','spectral_flux','onset_strength'\n",
    "]\n",
    "\n",
    "midi_columns = [\n",
    "    'device_id','channel','note','velocity',\n",
    "    'cc_number','cc_value','program_number','type'\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Fill missing numeric values\n",
    "# -----------------------------\n",
    "timeline_df[audio_columns + midi_columns[:-1]] = (\n",
    "    timeline_df[audio_columns + midi_columns[:-1]].fillna(0)\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Encode MIDI type\n",
    "# -----------------------------\n",
    "type_encoder = LabelEncoder()\n",
    "timeline_df['type_enc'] = type_encoder.fit_transform(\n",
    "    timeline_df['type'].fillna('none')\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Feature set\n",
    "# -----------------------------\n",
    "feature_columns = audio_columns + midi_columns[:-1] + ['type_enc']\n",
    "\n",
    "# -----------------------------\n",
    "# Targets (next event)\n",
    "# -----------------------------\n",
    "timeline_df['next_channel']   = timeline_df['channel'].shift(-1)\n",
    "timeline_df['next_cc_number'] = timeline_df['cc_number'].shift(-1)\n",
    "timeline_df['next_cc_value']  = timeline_df['cc_value'].shift(-1)\n",
    "\n",
    "timeline_df = timeline_df.dropna(\n",
    "    subset=['next_channel','next_cc_number','next_cc_value']\n",
    ")\n",
    "\n",
    "X = timeline_df[feature_columns]\n",
    "y = timeline_df[['next_channel','next_cc_number','next_cc_value']]\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Gradient Boosted Trees\n",
    "# -----------------------------\n",
    "gb_model = MultiOutputClassifier(\n",
    "    HistGradientBoostingClassifier(\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate\n",
    "# -----------------------------\n",
    "score = gb_model.score(X_test, y_test)\n",
    "print(\"Gradient Boost multi-output accuracy:\", score)\n",
    "\n",
    "# -----------------------------\n",
    "# Example prediction\n",
    "# -----------------------------\n",
    "example_row = X_test.iloc[[0]]\n",
    "pred = gb_model.predict(example_row)\n",
    "\n",
    "print(\"Predicted next MIDI (channel, cc_number, cc_value):\", pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffcc8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e7e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "audio_columns = [\n",
    "    'rms_db','rms_delta','centroid','rolloff','flatness',\n",
    "    'low','mid','high','spectral_flux','onset_strength'\n",
    "]\n",
    "\n",
    "midi_columns = ['channel','cc_number','cc_value','type']\n",
    "\n",
    "df = timeline_df.copy()\n",
    "\n",
    "df[audio_columns + ['channel','cc_number','cc_value']] = (\n",
    "    df[audio_columns + ['channel','cc_number','cc_value']]\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "type_encoder = LabelEncoder()\n",
    "df['type_enc'] = type_encoder.fit_transform(df['type'].fillna('none'))\n",
    "\n",
    "feature_columns = audio_columns + ['channel','cc_number','cc_value','type_enc']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "\n",
    "\n",
    "SEQ_LEN = 16   # try 8, 16, 32 later\n",
    "\n",
    "X_seq = []\n",
    "y_channel = []\n",
    "y_cc = []\n",
    "y_value = []\n",
    "\n",
    "for i in range(len(df) - SEQ_LEN - 1):\n",
    "    window = df.iloc[i:i+SEQ_LEN]\n",
    "\n",
    "    X_seq.append(window[feature_columns].values)\n",
    "\n",
    "    y_channel.append(df.iloc[i+SEQ_LEN]['channel'])\n",
    "    y_cc.append(df.iloc[i+SEQ_LEN]['cc_number'])\n",
    "    y_value.append(df.iloc[i+SEQ_LEN]['cc_value'])\n",
    "\n",
    "X_seq = np.array(X_seq, dtype=np.float32)\n",
    "y_channel = np.array(y_channel, dtype=np.int64)\n",
    "y_cc = np.array(y_cc, dtype=np.int64)\n",
    "y_value = np.array(y_value, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d785e618",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[8], line 70\u001b[0m\n",
      "\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m     66\u001b[0m pred_ch, pred_cc, pred_val \u001b[38;5;241m=\u001b[39m model(Xb)\n",
      "\u001b[1;32m     68\u001b[0m loss \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m     69\u001b[0m     loss_channel(pred_ch, ch) \u001b[38;5;241m+\u001b[39m\n",
      "\u001b[0;32m---> 70\u001b[0m     \u001b[43mloss_cc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_cc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n",
      "\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m loss_value(pred_val, val)\n",
      "\u001b[1;32m     72\u001b[0m )\n",
      "\u001b[1;32m     74\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1310\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n",
      "\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m-> 1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3462\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n",
      "\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   3461\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "\u001b[0;32m-> 3462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MidiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.channel_head = nn.Linear(hidden_dim, 16)   # MIDI channels\n",
    "        self.cc_head = nn.Linear(hidden_dim, 128)       # CC numbers\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)      # CC value (regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]  # last timestep\n",
    "\n",
    "        return (\n",
    "            self.channel_head(h),\n",
    "            self.cc_head(h),\n",
    "            self.value_head(h).squeeze(-1)\n",
    "        )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = MidiLSTM(input_dim=X_seq.shape[2]).to(device)\n",
    "\n",
    "loss_channel = nn.CrossEntropyLoss()\n",
    "loss_cc = nn.CrossEntropyLoss()\n",
    "loss_value = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_seq),\n",
    "    torch.tensor(y_channel),\n",
    "    torch.tensor(y_cc),\n",
    "    torch.tensor(y_value)\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for Xb, ch, cc, val in loader:\n",
    "        Xb = Xb.to(device)\n",
    "        ch = ch.to(device)\n",
    "        cc = cc.to(device)\n",
    "        val = val.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_ch, pred_cc, pred_val = model(Xb)\n",
    "\n",
    "        loss = (\n",
    "            loss_channel(pred_ch, ch) +\n",
    "            loss_cc(pred_cc, cc) +\n",
    "            0.1 * loss_value(pred_val, val)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss={total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7149d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f28ba687",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target -1 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n",
      "\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m     13\u001b[0m pred_ch, pred_cc, pred_val \u001b[38;5;241m=\u001b[39m model(Xb)\n",
      "\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m     16\u001b[0m     loss_channel(pred_ch, ch) \u001b[38;5;241m+\u001b[39m\n",
      "\u001b[0;32m---> 17\u001b[0m     \u001b[43mloss_cc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_cc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m\n",
      "\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m loss_value(pred_val, val)\n",
      "\u001b[1;32m     19\u001b[0m )\n",
      "\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1310\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n",
      "\u001b[1;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "\u001b[0;32m-> 1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Desktop/Local Files/Working_JGMBAirLocalWorking/Github Code/Deep-Improv/Code/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3462\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n",
      "\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m   3461\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n",
      "\u001b[0;32m-> 3462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   3469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: Target -1 is out of bounds."
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for Xb, ch, cc, val in loader:\n",
    "        Xb = Xb.to(device)\n",
    "        ch = ch.to(device)\n",
    "        cc = cc.to(device)\n",
    "        val = val.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_ch, pred_cc, pred_val = model(Xb)\n",
    "\n",
    "        loss = (\n",
    "            loss_channel(pred_ch, ch) +\n",
    "            loss_cc(pred_cc, cc) +\n",
    "            0.1 * loss_value(pred_val, val)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    example = torch.tensor(X_seq[-1:]).to(device)\n",
    "    ch_pred, cc_pred, val_pred = model(example)\n",
    "\n",
    "    print(\"Predicted channel:\", ch_pred.argmax(dim=1).item())\n",
    "    print(\"Predicted CC:\", cc_pred.argmax(dim=1).item())\n",
    "    print(\"Predicted CC value:\", int(val_pred.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b80210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted channel: 6\n",
      "Predicted CC: 12\n",
      "Predicted CC value: 3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e3d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
